{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "nspDOXuI3Bu7"
      ],
      "mount_file_id": "11iOwp5ExjZ5hcwK98UbJVDd1OlTT5u9F",
      "authorship_tag": "ABX9TyPQYBuIWcDS/VVNWXqAAgh8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingsgameBen/Lab1VacSkills/blob/main/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzSUZyLB2JjW"
      },
      "source": [
        "# Lab1 爬蟲專題 \n",
        "###人力銀行python職缺-工作技能統計\n",
        "\n",
        "1. 分別下載1111、104人力銀行python職缺\n",
        "    儲存成.json、.csv 、.xlsx檔\n",
        "2. 分析統計英文的技能terminology\n",
        "3. 依照terms出現次數產生文字雲"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UNiKun-2qUi"
      },
      "source": [
        "# 1111人力銀行爬蟲"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Hhwgdu2HLw"
      },
      "source": [
        "# 1111人力銀行\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "# MAC + Pycharm: SSL Certification failed 驗證機制較嚴苛 須證書代碼\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "\n",
        "table = {\n",
        "        \"job_name\": [],\n",
        "        \"job_href\": [],\n",
        "        \"update_date\": [],\n",
        "        \"company\": [],\n",
        "        \"com_area\": [],\n",
        "        \"industry\": [],\n",
        "        \"exp_require\": [],\n",
        "        \"edu_require\": [],\n",
        "        \"skills\": [],\n",
        "        \"dept_require\": [],\n",
        "        \"salary\": [],\n",
        "        \"job_content\": [],\n",
        "        \"job_type\": [],\n",
        "        \"benefit_tags\": [],\n",
        "        \"benefit_desc\": []\n",
        "    }\n",
        "# .csv資料夾名稱\n",
        "DIR = \"./1111\"\n",
        "\n",
        "\n",
        "def vacancy_1111():\n",
        "    BASE_DIR = \"/content/drive/MyDrive/Lab1/1111\"\n",
        "    # 計數職缺分頁數、職缺項目數\n",
        "    pagination = 1\n",
        "    count = 1\n",
        "\n",
        "    # 搜尋結果第一頁(20筆),放在while loop 外以避免重複比對執行\n",
        "    url = \"https://www.1111.com.tw/search/job?ks=python\"\n",
        "    response = requests.get(url).text\n",
        "    html = BeautifulSoup(response, features=\"html.parser\")\n",
        "    # 取得最後一頁, 3/12網站更改class naming\n",
        "    # final_page = html.select('div.adv-footer > div.container > div.adv-footer__content > div.adv-footer__content-item > select > option:nth-last-of-type(1)')[0]['value']\n",
        "    final_page = html.find(\"div\", class_=\"srh-footer\").select(\n",
        "        'div.container > div.srh-footer__content > div.srh-footer__content-item > select > option')[-1]['value']\n",
        "    final_page = int(final_page)\n",
        "    while True:\n",
        "        if pagination != 1:\n",
        "            url = \"https://www.1111.com.tw/search/job?ks=python&fs=1&page=\" + str(pagination) + \"&act=load_page\"\n",
        "            response = requests.get(url).text\n",
        "            response = json.loads(response)['html']\n",
        "            html = BeautifulSoup(response, features=\"html.parser\")\n",
        "        print(pagination, '/', final_page)\n",
        "\n",
        "        rs = html.find_all(\"div\", class_=\"item__job-info\")\n",
        "        for r in rs:\n",
        "            # 職缺連結:-> str\n",
        "            job_href = r.find(\"a\")['href'].strip()\n",
        "            response = requests.get(job_href).text\n",
        "            html = BeautifulSoup(response, features=\"html.parser\")\n",
        "            # 職缺名稱:-> str\n",
        "            job_name = html.find('div', class_=\"vacancy-header-row\").find('h1').text.strip()\n",
        "            # 更新日: -> str\n",
        "            update_date = html.find('div', class_=\"vacancy-header-row\").find('p', class_='update').text.split(\"：\")[-1]\n",
        "            # 公司名稱:-> str\n",
        "            company = html.find('div', class_=\"vacancy-company-text\").find('h2').find('a').text.strip()\n",
        "            # 公司產業別: -> str\n",
        "            if html.find('div', class_=\"vacancy-company-text\").find('div', class_=\"company-category\"):\n",
        "                industry = html.find('div', class_=\"vacancy-company-text\").find('div', class_=\"company-category\").find('a').text.strip()\n",
        "            else:\n",
        "                # 若顯示頁面找不到產業類別,則從原始碼的特定script標籤中取出\n",
        "                desc = json.loads(html.find_all(\"script\", {\"type\": \"application/ld+json\"})[1].contents[0])[0]\n",
        "                industry = desc['industry']\n",
        "            main_contents = html.find('div', class_=\"vacancy-header-row\").find('ul', class_=\"vacancy-description-main\").find_all('p')\n",
        "\n",
        "            # 取出職缺主要項目( 薪資、 工作經驗、 學歷、 公司地區 ):-> str\n",
        "            salary, exp_require, edu_require, com_area = \"\", \"\", \"\", \"\"\n",
        "            # 用包含式逐項取出職缺主要項目內容依序放入清單的變數中\n",
        "            [salary, exp_require, edu_require, com_area] = [spans.select('span:nth-of-type(2)')[0].text.strip() for spans in main_contents]\n",
        "\n",
        "            # 先找到職缺詳細內容的大區塊作為基準點,在依此尋找各項資料\n",
        "            all_info = html.find('section', id=\"Job-Detail\").find('div', class_=\"container\").find('div', class_=\"row\").find('div', id=\"job-detail-info\")\n",
        "            # 工作內容\n",
        "            detail_block = all_info.find('div', class_=\"job-detail-info\").find('div', class_=\"job-detail-info-content\").find_all(\"p\")\n",
        "            # 用包含式將工作內容區塊的所有 段落標籤 移除: -> list\n",
        "            job_content = [con.text.strip() for con in detail_block]\n",
        "            # 工作制度/性質\n",
        "            type_block = all_info.select(\".job-detail-panel > .job-detail-panel-content\")[0]\n",
        "            # 用包裹式取出 職務類別 的清單:-> list\n",
        "            job_type = [a['title'] for a in type_block.select(\"dl > dd > span.category > a\")]\n",
        "            # 要求條件\n",
        "            # 條件標題:-> list\n",
        "            require_titles = all_info.select(\".job-detail-panel > .job-detail-panel-content\")[2].find('dl').find_all('dt')\n",
        "            # print([t.text.strip(':') for t in require_titles])\n",
        "            require_block = all_info.select(\".job-detail-panel > .job-detail-panel-content\")[2].find('dl')\n",
        "            # 科系限制:->str(不拘) or list\n",
        "            departments = require_block.select('dd > span')[3]\n",
        "            # 若有科系要求則以 list 存入,不拘 則直接以 str 儲存\n",
        "            if departments.find_all('a'):\n",
        "                dept_require = [dept['title'] for dept in departments.find_all('a')]\n",
        "            else:\n",
        "                dept_require = departments.text.strip()\n",
        "            # 其他要求條件(0~多項條件):-> dict\n",
        "            skills = {}\n",
        "            if len(require_titles) > 4:\n",
        "                # 去除 latin1的不間斷空格、\\r\\n換行、\\t縮排、縮減大量空格\n",
        "                extra_requires = [dd.text.replace('\\xa0', '').replace('\\r', '').replace('\\n', '').replace('\\t', '').replace('     ', '').strip() for dd in require_block.select('dt + dd')[4:]]\n",
        "                for i in require_titles[4:]:\n",
        "                    idx = i.text.strip('：')\n",
        "                    skills[idx] = extra_requires[require_titles.index(i)-4]\n",
        "            # 福利制度\n",
        "            benefit_tags, benefit_desc = \"\", \"\"\n",
        "            if all_info.find('div', id='service-button-point'):\n",
        "                benefit_block = all_info.find('div', id='service-button-point').find('div', class_='job-detail-panel-content')\n",
        "                # 福利制度的標籤(不抓取法定項目的福利):-> list\n",
        "                if benefit_block.select('a'):\n",
        "                    benefit_tags = [item['title'] for item in benefit_block.select('a') if item['title'] != 'legal']\n",
        "                # 其他福利制度的說明:->str\n",
        "                if benefit_block.select('div.list-box-full > p'):\n",
        "                    benefit_desc = benefit_block.select('div.list-box-full > p')[0].text.strip()\n",
        "\n",
        "            table['job_name'].append(job_name)\n",
        "            table['job_href'].append(job_href)\n",
        "            table['update_date'].append(update_date)\n",
        "            table['company'].append(company)\n",
        "            table['industry'].append(industry)\n",
        "            table['com_area'].append(com_area)\n",
        "            table['edu_require'].append(edu_require)\n",
        "            table['exp_require'].append(exp_require)\n",
        "            table['salary'].append(salary)\n",
        "            table['job_content'].append(job_content)\n",
        "            table['job_type'].append(job_type)\n",
        "            table['dept_require'].append(dept_require)\n",
        "            table['skills'].append(skills)\n",
        "            table['benefit_tags'].append(benefit_tags)\n",
        "            table['benefit_desc'].append(benefit_desc)\n",
        "\n",
        "            # print()\n",
        "            print(count, '職缺：', job_name, job_href)\n",
        "            # print('工作：')\n",
        "            # print('更新日', update_date)\n",
        "            # print('公司,產業別：', company, industry)\n",
        "            # print('地點：', com_area)\n",
        "            # print(edu_require)\n",
        "            # print(exp_require)\n",
        "            # print(salary)\n",
        "            # print(job_content)\n",
        "            # print(job_type)\n",
        "            # print(dept_require)\n",
        "            # print(skills)\n",
        "            # print(benefit_tags)\n",
        "            # print(benefit_desc)\n",
        "            print('--'*100)\n",
        "            count = count + 1\n",
        "\n",
        "        pagination = pagination + 1\n",
        "        # 判斷是否超過最終頁, 若超過則跳出for loop 及 while loop\n",
        "        if pagination > final_page:\n",
        "            file_name = \"vacancy_python\"\n",
        "            if not os.path.exists(BASE_DIR):\n",
        "                os.makedirs(BASE_DIR)\n",
        "            df = pd.DataFrame(table)\n",
        "            csv_fn = BASE_DIR+\"/\"+file_name+\".csv\"\n",
        "            df.to_csv(csv_fn, encoding=\"utf-8\", index=False)\n",
        "            # 儲存JSON資料(一個JSON檔儲存全部職缺)\n",
        "            json_fn = BASE_DIR + \"/\" + file_name + \".json\"\n",
        "            with open(json_fn, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(table, f, ensure_ascii=False, indent=4)\n",
        "            print(\"=\"*30, \"Procedure End\", \"=\"*30)\n",
        "\n",
        "            excel_fn = BASE_DIR+\"/\"+file_name+\".xlsx\"\n",
        "            df.to_excel(excel_fn)\n",
        "            count = count - 1\n",
        "            return count\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE4zI_1l23bP"
      },
      "source": [
        "# 104人力銀行爬蟲"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kU7XpxI2HhL"
      },
      "source": [
        "# 104銀行 python相關職缺\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "table = {\n",
        "        \"job_name\": [],\n",
        "        \"job_href\": [],\n",
        "        \"update_date\": [],\n",
        "        \"company\": [],\n",
        "        \"com_area\": [],\n",
        "        \"industry\": [],\n",
        "        \"exp_require\": [],\n",
        "        \"edu_require\": [],\n",
        "        \"skills\": [],\n",
        "        \"dept_require\": [],\n",
        "        \"salary\": [],\n",
        "        \"job_content\": [],\n",
        "        \"job_type\": [],\n",
        "        \"benefit_tags\": [],\n",
        "        \"benefit_desc\": []\n",
        "    }\n",
        "DIR = \"./104\"\n",
        "\n",
        "\n",
        "def vacancy_104():\n",
        "    BASE_DIR = \"/content/drive/MyDrive/Lab1/104\"\n",
        "    # 計數職缺分頁數、職缺項目數\n",
        "    pagination = 115\n",
        "    count = 1\n",
        "    while True:\n",
        "        # 只取得工作經歷<1年的python職缺\n",
        "        url_1year = \"https://www.104.com.tw/jobs/search/?keyword=python&page=\" + str(pagination) + \"&jobexp=1\"\n",
        "        response = requests.get(url_1year).text\n",
        "        html = BeautifulSoup(response, features=\"html.parser\")\n",
        "\n",
        "        # 取出第2個沒有src屬性的script標籤,其內容中有totalPage的總頁數資料,\n",
        "        # 目前無法透過.text或.contents取得totalPage, 改成用while True + 該page取不到ResultSet來終止迴圈\n",
        "        rs = html.find(\"div\", id=\"main-content\").find(\"div\", id=\"js-job-content\").select(\"article.job-list-item\")\n",
        "        # 以還能不能取得職缺項目, 判斷是否停止搜尋, 停止while迴圈前儲存所有資料\n",
        "        if not rs:\n",
        "            file_name = \"vacancy_python\"\n",
        "            csv_fn = BASE_DIR +\"/\"+file_name+\".csv\"\n",
        "            if not os.path.exists(BASE_DIR):\n",
        "                os.makedirs(BASE_DIR)\n",
        "            df = pd.DataFrame(table)\n",
        "            df.to_csv(csv_fn, encoding=\"utf-8\", index=False)\n",
        "            # 儲存JSON資料(一個JSON檔儲存全部職缺)\n",
        "            json_fn = BASE_DIR+\"/\"+file_name+\".json\"\n",
        "            with open(json_fn, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(table, f, ensure_ascii=False, indent=4)\n",
        "            print(\"=\"*30, \"Procedure End\", \"=\"*30)\n",
        "\n",
        "            excel_fn = BASE_DIR+\"/\"+file_name+\".xlsx\"\n",
        "            df.to_excel(excel_fn)\n",
        "            count = count - 1\n",
        "            return count\n",
        "\n",
        "        # start_time = time.time()\n",
        "        # 建立網址清單,使用grequests的imap()平行發送多個請求\n",
        "        # href_ls = [\"https:\" + r.find(\"a\")['href'].strip() for r in rs]\n",
        "        # reqs = [grequests.get(href) for href in href_ls]\n",
        "        # responses = grequests.imap(reqs, grequests.Pool(len(reqs)))\n",
        "        for r in rs:\n",
        "            # 直接取得父元素下第一個取得的<a>, 職缺連結:-> str\n",
        "            job_href = \"https:\" + r.find(\"a\")['href'].strip()\n",
        "            response = requests.get(job_href).text\n",
        "            html = BeautifulSoup(response, features=\"html.parser\")\n",
        "            # print(html)\n",
        "            titles = html.find(\"title\").text.split(\"｜\")\n",
        "            # 職缺名稱\n",
        "            job_name = titles[0].strip()\n",
        "            # 公司名稱\n",
        "            company = titles[1].strip()\n",
        "            # 公司地區\n",
        "            com_area = titles[2].split(\"－\")[0]\n",
        "            # 網頁原始碼只有部分資料, 完整資料是透過json帶進來的,做法有別於1111, 須以JSON方式操作\n",
        "            # 以？分割並透過取段操作取得顯示網址列的職缺ID - 長度4~5碼英數字, 放入職缺內容的JSON真實網址\n",
        "            json_id = job_href.split(\"?\")[0].split(\"/\")[-1]\n",
        "            json_url = \"https://www.104.com.tw/job/ajax/content/\" + json_id\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36',\n",
        "                'Referer': 'https://www.104.com.tw/job/' + json_id + '?jobsource=jolist_a_relevance'\n",
        "            }\n",
        "\n",
        "            response = requests.get(json_url, headers=headers)\n",
        "            json_data = json.loads(response.text)[\"data\"]\n",
        "            # 更新日期、產業別、學歷條件、工作經驗、薪資、工作內容、職務類別、科系條件、工作技能、福利制度\n",
        "            update_date = json_data[\"header\"][\"appearDate\"]\n",
        "            industry = json_data[\"industry\"]\n",
        "            edu_require = json_data[\"condition\"][\"edu\"]\n",
        "            exp_require = json_data[\"condition\"][\"workExp\"]\n",
        "            salary = json_data[\"jobDetail\"][\"salary\"]\n",
        "            job_content = json_data[\"jobDetail\"][\"jobDescription\"].strip()\n",
        "            job_type = [cate[\"description\"] for cate in json_data[\"jobDetail\"][\"jobCategory\"]]\n",
        "            dept_require = json_data[\"condition\"][\"major\"]\n",
        "            skills = [special[\"description\"] for special in json_data[\"condition\"][\"specialty\"]]\n",
        "            benefit_desc = json_data[\"welfare\"][\"welfare\"]\n",
        "\n",
        "            table['job_name'].append(job_name)\n",
        "            table['job_href'].append(job_href)\n",
        "            table['update_date'].append(update_date)\n",
        "            table['company'].append(company)\n",
        "            table['industry'].append(industry)\n",
        "            table['com_area'].append(com_area)\n",
        "            table['edu_require'].append(edu_require)\n",
        "            table['exp_require'].append(exp_require)\n",
        "            table['salary'].append(salary)\n",
        "            table['job_content'].append(job_content)\n",
        "            table['job_type'].append(job_type)\n",
        "            table['dept_require'].append(dept_require)\n",
        "            table['skills'].append(skills)\n",
        "            table['benefit_tags'].append(None)\n",
        "            table['benefit_desc'].append(benefit_desc)\n",
        "\n",
        "            # print()\n",
        "            print('Page:', pagination)\n",
        "            print(count, job_name)\n",
        "            # print(job_href)\n",
        "            # print(update_date)\n",
        "            # print('公司,產業別：', company, industry)\n",
        "            # print('地點：', com_area)\n",
        "            # print(edu_require)\n",
        "            # print(exp_require)\n",
        "            # print(salary)\n",
        "            # print(job_content)\n",
        "            # print(job_type)\n",
        "            # print(dept_require)\n",
        "            # print(skills)\n",
        "            # print(benefit_desc)\n",
        "            print('-' * 100)\n",
        "            count = count + 1\n",
        "\n",
        "        pagination = pagination + 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM36nXL-86Bo"
      },
      "source": [
        "# main.py主程式\n",
        "分別呼叫1111、104爬蟲程式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsUB2G6r2FdF"
      },
      "source": [
        "# 抓取人力銀行職缺, 分析個別工作技能清單\n",
        "# import bank_104\n",
        "# import bank_1111\n",
        "import time\n",
        "from urllib import request\n",
        "import pandas as pd\n",
        "import re\n",
        "import jieba\n",
        "from jieba import analyse\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# The File size(>4.25MB) exceeds configured limit (2.56MB), code insight features not available\n",
        "# 日誌文件過大,超過默認設定,要增加限制容量並重啟pycharm ->Help->Edit Custom Properties->加入\"idea.max.intellisense.filesize=99999\"\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/2021_Python班/專題_爬蟲/Lab1\"\n",
        "\n",
        "# 爬取人力銀行(1111、104)職缺資料,儲存成csv、excel及json資料\n",
        "# 抓取104人力銀行職缺(工作經歷1年以下):\n",
        "def find_vacancy(bank):\n",
        "    start_time = time.time()\n",
        "    if bank == \"104\":\n",
        "        # vac_qty = bank_104.vacancy_104()\n",
        "        vac_qty = vacancy_104()\n",
        "    elif bank == \"1111\":\n",
        "        # vac_qty = bank_1111.vacancy_1111()\n",
        "        vac_qty = vacancy_1111()\n",
        "    end_time = time.time()\n",
        "    print(\"Download \", vac_qty, \"vacancies of\", bank, \". It takes\", round((end_time - start_time)/60, 2), 'minutes.')\n",
        "\n",
        "\n",
        "# 計算詞頻\n",
        "def count_seg_freq(seg_list):\n",
        "    seg_df = pd.DataFrame(seg_list, columns=['Terminology'])\n",
        "    seg_df['count'] = 1\n",
        "    sef_freq = seg_df.groupby('Terminology')['count'].sum().sort_values(ascending=False)\n",
        "    sef_freq = pd.DataFrame(sef_freq)\n",
        "    return sef_freq\n",
        "\n",
        "\n",
        "# 分析關鍵字\n",
        "def analyse_kw(mode=0, *mode_item):\n",
        "    # mode_dic = {0: '菜鳥', 1: '產業別', 2: '學士以下', 3: '不限科系', 4: '薪水4萬以下或面議', 5: '縣市別'}\n",
        "    url = \"https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big\"\n",
        "    request.urlretrieve(url, \"dict.big\")\n",
        "    jieba.set_dictionary(\"dict.big\")\n",
        "    jieba.load_userdict(BASE_DIR + \"/Dict/mix_terminologies.txt\")\n",
        "\n",
        "    total_vac = 0\n",
        "    eng_kw = []\n",
        "    bank_ls = [\"1111\", \"104\"]\n",
        "    # 不同的人力銀行分別分析\n",
        "    for bank in bank_ls:\n",
        "        # fn = bank + \"/vacancy_python.json\"\n",
        "        fn = \"/content/drive/MyDrive/Lab1/\" + bank + \"/vacancy_python.json\"\n",
        "        kws = set()\n",
        "        skill_all = []\n",
        "        count = 1\n",
        "        with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "            # 完整顯示所有資料列\n",
        "            pd.set_option('display.max_rows', None)\n",
        "            vacancy = pd.read_json(f)\n",
        "            df = pd.DataFrame(vacancy)\n",
        "\n",
        "        if mode == 1:\n",
        "            if mode_item:\n",
        "                mode_item = str(mode_item[0])\n",
        "                mask1 = df[\"industry\"].str.contains(mode_item)\n",
        "                mask2 = df[\"industry\"].notnull()\n",
        "                df = df[(mask1 & mask2)]\n",
        "            else:\n",
        "                mask1 = df[\"industry\"].notnull()\n",
        "                df = df[mask1]\n",
        "        elif mode == 2:\n",
        "            mask1 = df[\"edu_require\"].str.contains(\"不拘|高中|專科|大學\")\n",
        "            mask2 = df[\"edu_require\"].isnull()\n",
        "            df = df[(mask1 | mask2)]\n",
        "        elif mode == 3:\n",
        "            mask1 = df[\"dept_require\"].str.contains(\"不拘|商業|其他|一般|普通|軟體|網路\")\n",
        "            mask2 = df[\"dept_require\"].isna()\n",
        "            if bank == \"104\":\n",
        "                mask2 = df[\"dept_require\"].str.len() == 0\n",
        "                df = df[mask2]\n",
        "            else:\n",
        "                df = df[(mask1 | mask2)]\n",
        "        elif mode == 4:\n",
        "            mask1 = df[\"salary\"].str.contains(\"^月薪[23].,.*\")\n",
        "            mask2 = df[\"salary\"].str.contains(\"面議\")\n",
        "            # mask2 = df[\"salary\"].str.contains(\"年薪[4]??,*\")\n",
        "            df = df[(mask1 | mask2)]\n",
        "        elif mode == 5:\n",
        "            if mode_item:\n",
        "                mode_item = str(mode_item[0])\n",
        "                mask1 = df[\"com_area\"].str.contains(mode_item)\n",
        "            else:\n",
        "                mask1 = df[\"com_area\"].str.contains(\"台北市\")\n",
        "            df = df[mask1]\n",
        "        print(bank+\":\", df.shape)\n",
        "        print(\"_\"*100)\n",
        "\n",
        "        if bank == '1111':\n",
        "            df.set_index(\"exp_require\", inplace=True)\n",
        "            if mode == 0:\n",
        "                # 嘗試篩選出工作經驗2年以下及不拘的職缺\n",
        "                df = df.loc[[\"工作經歷-不拘\", \"1年以上工作經驗\"]]\n",
        "\n",
        "            total_vac += len(df.index)\n",
        "            for skill in df['skills']:\n",
        "                if type(skill) == dict:\n",
        "                    # 目前先過濾掉語言能力的要求(匹配忽略大小寫)\n",
        "                    values = [val for k, val in skill.items() if k != '語言能力']  # and not re.match(r'[a-z]+', val, re.I)]\n",
        "                else:\n",
        "                    values = skill\n",
        "                if not skill or not values:  # 略過空資料\n",
        "                    continue\n",
        "                skill_all += values\n",
        "        elif bank == '104':\n",
        "            total_vac += len(df.index)\n",
        "            for sk in df['skills']:\n",
        "                if sk:\n",
        "                    skill_all += sk\n",
        "\n",
        "        for val in skill_all:\n",
        "            val = val.strip().strip(\"..\").strip(\"...\").upper()\n",
        "            val = val.replace(\"FAMILIAR\", \"\").replace(\"COMPUTER\", \"\").replace(\"SYSTEM\", \"\").replace(\"WEB\", \"\").replace(\"EXPERIENCE\", \"\")\n",
        "            val = val.replace(\"ABILITY\", \"\").replace(\"JOB\", \"\").replace(\"KNOWLEDGE\", \"\").replace(\"SUCH\", \"\").replace(\"TOOLS\", \"\").replace(\"SIMILAR\", \"\")\n",
        "            val = val.replace(\"MS SQL\", \"MSSQL\")\n",
        "            jieba.analyse.set_stop_words(BASE_DIR + \"/Dict/stopwords.txt\")\n",
        "            jieba.add_word(\"MACHINE LEARNING\")\n",
        "            jieba.add_word(\"DEEP LEARNING\")\n",
        "            jieba.add_word(\"NODE.JS\")\n",
        "            jieba.suggest_freq(\"NODE.JS\", True)\n",
        "            if bank == '1111':\n",
        "                val = re.sub(r\"[Website]*?\\(?[http?]\\S+[/)$|/$]\", \"\", val, flags=re.IGNORECASE)  # 刪除連結URL\n",
        "                keywords = jieba.analyse.extract_tags(val, topK=30)\n",
        "                # print(\"\\n關鍵詞:\", count,  keywords)\n",
        "                for kw in keywords:\n",
        "                    if re.match(r'[A-Z]+', kw):\n",
        "                        eng_kw.append(kw)\n",
        "                    kws.add(kw)\n",
        "            elif bank == '104':\n",
        "                if re.match(r'[A-Z]+', val):\n",
        "                    eng_kw.append(val)\n",
        "\n",
        "            count = count + 1\n",
        "\n",
        "    word_freq = count_seg_freq(eng_kw)\n",
        "    freq_percent = round(word_freq[:200]/len(eng_kw)*100, 1)\n",
        "    freq_percent = freq_percent.rename(columns={\"count\": \"%\"})\n",
        "    word_freq = pd.concat([word_freq, freq_percent], sort=False, axis=1)\n",
        "    word_freq = word_freq.reset_index()\n",
        "    print(word_freq[:50])\n",
        "    print(\"Total terms:\", len(word_freq))\n",
        "    print(\"Filtered Vacancy:\", total_vac)\n",
        "    return eng_kw\n",
        "\n",
        "\n",
        "# 產生文字雲\n",
        "def word_cloud(mode=0, *mode_item):\n",
        "    eng_kw = analyse_kw(mode, *mode_item)\n",
        "    font_path = BASE_DIR + '/font/PatuaOne.ttf'\n",
        "    show_words = 200\n",
        "\n",
        "    # Default mode's image\n",
        "    back_path = BASE_DIR + '/Image/github.jpg'\n",
        "    fn_type = \"fresh_0_\"\n",
        "    if mode == 1:\n",
        "        back_path = BASE_DIR + '/Image/github2.jpeg'\n",
        "        fn_type = \"indus_1_\"\n",
        "        show_words = 150\n",
        "        if mode_item:\n",
        "            fn_type += mode_item[0] + \"_\"\n",
        "            show_words = 100\n",
        "    elif mode == 2:\n",
        "        back_path = BASE_DIR + '/Image/explosion.png'\n",
        "        fn_type = \"edu_2_\"\n",
        "        show_words = 150\n",
        "    elif mode == 3:\n",
        "        back_path = BASE_DIR + '/Image/riot.jpg'\n",
        "        fn_type = \"dept_3_\"\n",
        "        show_words = 100\n",
        "    elif mode == 4:\n",
        "        back_path = BASE_DIR + '/Image/python.jpg'\n",
        "        fn_type = \"pay_4_\"\n",
        "        show_words = 100\n",
        "    elif mode == 5:\n",
        "        back_path = BASE_DIR + '/Image/umu.jpeg'\n",
        "        fn_type = \"area_5_\"\n",
        "        show_words = 150\n",
        "        if mode_item:\n",
        "            fn_type += mode_item[0] + \"_\"\n",
        "            show_words = 100\n",
        "\n",
        "    fn = fn_type + back_path.split(\"/\")[-1].split(\".\")[0]+\".png\"\n",
        "    back_color = imageio.imread(back_path)\n",
        "    if not eng_kw:\n",
        "        eng_kw.append('EMPTY RESULT.查無職缺')\n",
        "    wc = WordCloud(\n",
        "        max_words=show_words,\n",
        "        mask=back_color,\n",
        "        font_path=font_path,\n",
        "        repeat=False,\n",
        "        collocations=False\n",
        "    ).generate_from_frequencies(Counter(eng_kw))\n",
        "    if mode in [2, 3, 4, 5]:\n",
        "        image_colors = ImageColorGenerator(back_color)\n",
        "        wc.recolor(color_func=image_colors)\n",
        "\n",
        "    if mode in [1, 5]:\n",
        "        plt.rcParams['font.sans-serif'] = 'STHeiti'\n",
        "    plt.legend(loc=1, title=fn[:-4], labels=fn_type.split(\"_\")[0])\n",
        "    plt.imshow(wc)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    if not os.path.exists(\"wordcloud_result\"):\n",
        "        os.mkdir(\"wordcloud_result\")\n",
        "    fig_path = \"wordcloud_result/\" + fn\n",
        "    wc.to_file(fig_path)  # 輸出圖片\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 2310筆 約26分,\n",
        "    # find_vacancy(\"104\")\n",
        "    # 893筆 約 15.6分, 908筆 約13分\n",
        "    # colab下載899筆 約5分鐘 3/20: 905筆約28分\n",
        "    # find_vacancy(\"1111\")\n",
        "\n",
        "    # mode_dic = {0: '菜鳥', 1: '產業別', 2: '學士以下', 3: '不限科系', 4: '薪水4萬以下或面議', 5: '縣市別'}\n",
        "    # print(analyse_kw())\n",
        "    # word_cloud()\n",
        "    # word_cloud(1)  # => 全部產業的職缺, terms統計前150項\n",
        "    # word_cloud(1, \"電腦\")  # => 產業別有“電腦” 的職缺, terms統計前100項\n",
        "    # word_cloud(1, \"光電|電子\")  # => 產業別有“光電”或“電子” 的職缺, terms統計前100項\n",
        "    # word_cloud(2)\n",
        "    # word_cloud(3)\n",
        "    # word_cloud(4)\n",
        "    # word_cloud(5)\n",
        "    word_cloud(5, \"桃園\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nspDOXuI3Bu7"
      },
      "source": [
        "# 工具程式 - 字典清單下載整併"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1BZdjf92HyN"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 從wikipedia取得程式語言清單關鍵字\n",
        "def programming_lang():\n",
        "    url = \"https://zh.wikipedia.org/wiki/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%88%97%E8%A1%A8\"\n",
        "    response = urlopen(url)\n",
        "    html = BeautifulSoup(response, features=\"html.parser\")\n",
        "    main_content = html.find('div', id='bodyContent').find('div', id='mw-content-text').find_all('div', class_='div-col')\n",
        "    a_ls = [tag.find_all('a') for tag in main_content]\n",
        "    programming_list = []\n",
        "    for anchors in a_ls:\n",
        "        for lang in anchors:\n",
        "            programming_list.append(lang.text)\n",
        "    if not os.path.exists('Dict'):\n",
        "        os.makedirs('Dict')\n",
        "    with open('Dict/programming.dict', 'w', encoding='utf-8')as f:\n",
        "        for text in programming_list:\n",
        "            f.write(text+'\\n')\n",
        "\n",
        "\n",
        "# 因為字詞來源於中國網站, 簡體字轉換成繁體字的操作, 暫時以手動方式轉換\n",
        "def operating_sys():\n",
        "    url = \"https://zh.wikipedia.org/wiki/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%88%97%E8%A1%A8\"\n",
        "    response = urlopen(url)\n",
        "    html = BeautifulSoup(response, features=\"html.parser\")\n",
        "    main_content = html.find('div', id='bodyContent').find('div', id='mw-content-text').find_all('ul')\n",
        "    a_ls = [li.find_all('a') for li in main_content]  # if not li.find('span')]  #不排除系統分類標題名稱\n",
        "    os_ls = []\n",
        "    for a in a_ls:\n",
        "        for system in a:\n",
        "            os_ls.append(system.text)\n",
        "    if not os.path.exists('Dict'):\n",
        "        os.makedirs('Dict')\n",
        "    with open('Dict/operating_sys.dict', 'w', encoding='utf-8')as f:\n",
        "        for text in os_ls:\n",
        "            f.write(text + '\\n')\n",
        "\n",
        "\n",
        "# 把字典內容去掉重複並全部轉成大寫\n",
        "def convert2upperset(file_name):\n",
        "    file_name = 'Dict/' + file_name + '.dict'\n",
        "    temp_ls = []\n",
        "    with open(file_name, 'r', encoding='utf-8')as f:\n",
        "        temp_ls = f.readlines()\n",
        "    upper_ls = set()\n",
        "    for temp in temp_ls:\n",
        "        upper_ls.add(temp.upper())\n",
        "    with open(file_name, 'w', encoding='utf-8')as f:\n",
        "        f.writelines(upper_ls)\n",
        "\n",
        "\n",
        "# 產業別列表\n",
        "def industry_list():\n",
        "    industry_set = set()\n",
        "    for bank in [\"1111\", \"104\"]:\n",
        "        file_name = \"../\" + bank + \"/vacancy_python.json\"\n",
        "        with open(file_name, 'r', encoding='utf-8')as f:\n",
        "            vacancy = pd.read_json(f)\n",
        "        for indus in vacancy['industry']:\n",
        "            industry_set.add(indus)\n",
        "        print(bank, \":\\n\", industry_set, \"\\n AccumulatedQty:\", len(industry_set))\n",
        "    if not os.path.exists(\"Dict\"):\n",
        "        os.makedirs(\"Dict\")\n",
        "    with open(\"Dict/industry_list.txt\", 'w', encoding='utf-8')as f:\n",
        "        for ind in industry_set:\n",
        "            if ind:  # 避免寫入None\n",
        "                f.write(ind+'\\n')\n",
        "    print(len(industry_set))\n",
        "\n",
        "\n",
        "# 學歷要求列表\n",
        "def industry_list():\n",
        "    edu_set = set()\n",
        "    for bank in [\"1111\", \"104\"]:\n",
        "        file_name = \"../\" + bank + \"/vacancy_python.json\"\n",
        "        with open(file_name, 'r', encoding='utf-8')as f:\n",
        "            vacancy = pd.read_json(f)\n",
        "        for indus in vacancy['edu_require']:\n",
        "            edu_set.add(indus)\n",
        "        print(bank, \":\\n\", edu_set, \"\\n AccumulatedQty:\", len(edu_set))\n",
        "    if not os.path.exists(\"Dict\"):\n",
        "        os.makedirs(\"Dict\")\n",
        "    edu_ls = sorted(edu_set)\n",
        "    with open(\"Dict/education_list.txt\", 'w', encoding='utf-8')as f:\n",
        "        for edu in edu_ls:\n",
        "            if edu:  # 避免寫入None\n",
        "                f.write(edu+'\\n')\n",
        "    print(len(edu_ls))\n",
        "\n",
        "\n",
        "# 科系要求列表\n",
        "def dept_list():\n",
        "    dept_set = set()\n",
        "    for bank in [\"1111\", \"104\"]:\n",
        "        file_name = \"../\" + bank + \"/vacancy_python.json\"\n",
        "        with open(file_name, 'r', encoding='utf-8')as f:\n",
        "            vacancy = pd.read_json(f)\n",
        "        for dept in vacancy['dept_require']:\n",
        "            for d in dept:\n",
        "                dept_set.add(d)\n",
        "        print(bank, \":\\n\", dept_set, \"\\n AccumulatedQty:\", len(dept_set))\n",
        "    if not os.path.exists(\"Dict\"):\n",
        "        os.makedirs(\"Dict\")\n",
        "    dept_ls = sorted(dept_set)\n",
        "    with open(\"Dict/department_list.txt\", 'w', encoding='utf-8')as f:\n",
        "        for dept in dept_ls:\n",
        "            f.write(dept+'\\n')\n",
        "    print(len(dept_ls))\n",
        "\n",
        "\n",
        "# 所有技能列表(from人力銀行的技能條件)\n",
        "def skills_list():\n",
        "    skills_set = set()\n",
        "    for bank in [\"1111\", \"104\"]:\n",
        "        file_name = \"../\" + bank + \"/vacancy_python.json\"\n",
        "        with open(file_name, 'r', encoding='utf-8')as f:\n",
        "            vacancy = pd.read_json(f)\n",
        "        for skills in vacancy['skills']:\n",
        "            for skill in skills:\n",
        "                skills_set.add(skill.strip().upper())\n",
        "        print(bank, \":\\n\", skills_set, \"\\n AccumulatedQty:\", len(skills_set))\n",
        "\n",
        "    if not os.path.exists(\"Dict\"):\n",
        "        os.makedirs(\"Dict\")\n",
        "    skills_ls = sorted(skills_set)\n",
        "    with open(\"Dict/terminology_list.txt\", 'w', encoding='utf-8')as f:\n",
        "        for skill in skills_ls:\n",
        "            if skill:  # 避免寫入None\n",
        "                f.write(skill+'\\n')\n",
        "    print(len(skills_ls))\n",
        "\n",
        "\n",
        "# 整合成一份所有技能列表(programming.dict、operating_sys.dict、terminology_list.txt => mix_terminologies.txt)\n",
        "def integrated_term_list():\n",
        "    terms_set = set()\n",
        "    for dic in [\"operating_sys.dict\", \"programming.dict\", \"terminology_list.txt\"]:\n",
        "        file_name = \"Dict/\" + dic\n",
        "        with open(file_name, 'r', encoding='utf-8')as f:\n",
        "            terms_ls = f.readlines()\n",
        "        for term in terms_ls:\n",
        "            terms_set.add(term.strip(\"\\n\").upper())\n",
        "        print(dic, \":\\n\", terms_set, \"\\n AccumulatedQty:\", len(terms_set))\n",
        "\n",
        "    terms_ls = sorted(terms_set)\n",
        "    with open(\"Dict/mix_terminologies.txt\", 'w', encoding='utf-8')as f:\n",
        "        for term in terms_ls:\n",
        "            if term:  # 避免寫入None\n",
        "                f.write(term+'\\n')\n",
        "    print(len(terms_ls))\n",
        "\n",
        "\n",
        "# 薪資待遇列表\n",
        "def sal_list():\n",
        "    sal_set = set()\n",
        "    for bank in [\"1111\", \"104\"]:\n",
        "        file_name = \"../\" + bank + \"/vacancy_python.json\"\n",
        "        with open(file_name, 'r', encoding='utf-8')as f:\n",
        "            vacancy = pd.read_json(f)\n",
        "        for indus in vacancy['salary']:\n",
        "            sal_set.add(indus)\n",
        "        print(bank, \":\\n\", sal_set, \"\\n AccumulatedQty:\", len(sal_set))\n",
        "    sal_ls = sorted(sal_set)\n",
        "    with open(\"Dict/salary_list.txt\", 'w', encoding='utf-8')as f:\n",
        "        for sal in sal_ls:\n",
        "            if sal:  # 避免寫入None\n",
        "                f.write(sal+'\\n')\n",
        "    print(len(sal_ls))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}